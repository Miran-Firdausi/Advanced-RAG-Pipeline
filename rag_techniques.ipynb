{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3afe8718",
   "metadata": {},
   "source": [
    "# Naive (Basic) RAG\n",
    "Uses simplistic retrieval methods, such as keyword matching, semantic similarity using Vector DB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a48a56",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "logged_retriever = retriever | RunnableLambda(log_chunks) | format_docs\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": logged_retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c911d628",
   "metadata": {},
   "source": [
    "# Simple RAG with Memory\n",
    "Integrating a storage component, allowing the model to retain information from previous interactions. This addition was crucial for enabling contextual awareness across multiple queries, particularly in continuous conversations, often achieved through prompt caching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "815ef33c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Add history using memory\n",
    "session_histories = {}\n",
    "\n",
    "def get_session_history(session_id: str):\n",
    "    if session_id not in session_histories:\n",
    "        session_histories[session_id] = InMemoryChatMessageHistory()\n",
    "    return session_histories[session_id]\n",
    "\n",
    "# Extract question and history to pass to retriever and prompt\n",
    "def input_mapper(input):\n",
    "    return {\n",
    "        \"context\": logged_retriever.invoke(input[\"question\"]),\n",
    "        \"question\": input[\"question\"],\n",
    "        \"history\": input[\"history\"]\n",
    "    }\n",
    "\n",
    "# Chain\n",
    "rag_chain = (\n",
    "    RunnableLambda(input_mapper)\n",
    "    | CHAT_HISTORY_PROMPT\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "rag_with_history = RunnableWithMessageHistory(\n",
    "    runnable=rag_chain,\n",
    "    get_session_history=get_session_history,\n",
    "    input_messages_key=\"question\",\n",
    "    history_messages_key=\"history\",\n",
    "    output_messages_key=\"output\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d36ea00",
   "metadata": {},
   "source": [
    "# Branched RAG\n",
    "Designed for improved efficiency, Branched RAG allows the system to evaluate the user query and intelligently select the most relevant specific data source(s) to query, rather than indiscriminately querying all available sources. This targeted approach streamlines retrieval and reduces computational overhead."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa92c5f",
   "metadata": {},
   "source": [
    "# HyDE (Hypothetical Document Embeddings)\n",
    "This technique significantly improved retrieval relevance. Instead of directly retrieving documents, HyDE first creates an embedded representation of what an ideal document might look like, given the query. This hypothetical document then guides the actual retrieval process, leading to more precise and higher-quality results by addressing query-document asymmetry."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f29f3f6",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
