{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3afe8718",
   "metadata": {},
   "source": [
    "# Naive (Basic) RAG\n",
    "Uses simplistic retrieval methods, such as keyword matching, semantic similarity using Vector DB."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c911d628",
   "metadata": {},
   "source": [
    "# Simple RAG with Memory\n",
    "Integrating a storage component, allowing the model to retain information from previous interactions. This addition was crucial for enabling contextual awareness across multiple queries, particularly in continuous conversations, often achieved through prompt caching"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d36ea00",
   "metadata": {},
   "source": [
    "# Branched RAG\n",
    "Designed for improved efficiency, Branched RAG allows the system to evaluate the user query and intelligently select the most relevant specific data source(s) to query, rather than indiscriminately querying all available sources. This targeted approach streamlines retrieval and reduces computational overhead."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa92c5f",
   "metadata": {},
   "source": [
    "# HyDE (Hypothetical Document Embeddings)\n",
    "This technique significantly improved retrieval relevance. Instead of directly retrieving documents, HyDE first creates an embedded representation of what an ideal document might look like, given the query. This hypothetical document then guides the actual retrieval process, leading to more precise and higher-quality results by addressing query-document asymmetry."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f29f3f6",
   "metadata": {},
   "source": [
    "## To Explore\n",
    "- Agentic Rag\n",
    "- Caching layer\n",
    "    - Use hashing to check existing processed documents.\n",
    "    - Use Redis to store hashes in memory.\n",
    "- RAG vs CAG vs KAG\n",
    "- Self RAG\n",
    "- Langchain caching\n",
    "- When things are too complicated in PDF, directly pass the page screenshot to the LLM\n",
    "- Use Hybrid LLM, Gemini for basic reasoning. OpenAI for more advanced comprehension\n",
    "\n",
    "## Query Translation\n",
    "Modifying the user query to make it more suitable for retrieval.\n",
    "Multi-query, RAG-Fusion, Decomposition, Step-back, HyDe\n",
    "\n",
    "## Routing\n",
    "Taking natural language and converting it into a form that makes it easy to route it to the right source.\n",
    "Choosing the right DB, \n",
    "\n",
    "## Query Construction\n",
    "Text to SQL, Text to Metadata for Vector DB (Autogenerates metadata filters from query).\n",
    "\n",
    "## Indexing\n",
    "\n",
    "## Ranking\n",
    "Rerank or filter retrieved documents.\n",
    "Active RAG: If retrieved documents are not relevant, re-retrieve or retrieve from new data source.\n",
    "CRAG: Fact-checking, self-correcting mechanism\n",
    "\n",
    "## Generation\n",
    "\n",
    "\n",
    "![RAG Overview](images/rag-overview.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b12aaef1",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
